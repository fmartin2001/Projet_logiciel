{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a2c6e1",
   "metadata": {},
   "source": [
    "# Création d'une base de donnée équilibrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2767b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47e34b",
   "metadata": {},
   "source": [
    "Après des premiers test avec notre autoencodeur, nous nous sommes rendus compte de plusieurs chose. La base de données est composées de plus de femmes que d'hommes. Cette composition implique que notre autoencodeur reconnaissait des femmes même quand l'image d'origine était un homme.\n",
    "Pour résoudre ce problème et éviter que ce problème se répètes avec d'autre attributs nous avons créer une bases de données ou chaque attributs est représenté au moins un certain nombre de fois et le nombre d'hommes et de femmes est sur tout les attributs le même.\n",
    "Nous avons aussi séparer la base de données en plus petits dataset de manière à ce que l'entrainement puissent se faire plus facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665a54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs_images = pds.read_csv(\"new_list_attr_celba.csv\", sep=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c6485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compte_homme_femme(dataframe):\n",
    "    \"\"\"Permet de compter le nombre d'hommes et de femmes dans le dataset\n",
    "    Parameters:\n",
    "        dataframe (pandas.Dataframe): Dataframe contenant les différents attributs par images avec 1 quand l'attribut est présent et -1 quand il ne l'est pas\n",
    "    Return:\n",
    "        nombre_hommes (int): le nombre d'homme dans le dataset\n",
    "        nombre_femmes (int) : le nombre de femme dans le dataset\n",
    "    \"\"\"\n",
    "    nombre_hommes=0\n",
    "    nombre_femmes=0\n",
    "\n",
    "    for num_image in dataframe.index:\n",
    "            if dataframe['Male'][num_image] == 1:\n",
    "                nombre_hommes += 1\n",
    "            elif dataframe['Male'][num_image] == -1:\n",
    "                nombre_femmes += 1\n",
    "\n",
    "    return nombre_hommes, nombre_femmes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89c739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres d'hommes :  84434 \n",
      "Nombres de femmes :  118165\n"
     ]
    }
   ],
   "source": [
    "nombre_hommes, nombre_femmes =  compte_homme_femme(attributs_images)\n",
    "print(\"Nombres d'hommes : \", nombre_hommes, \"\\nNombres de femmes : \", nombre_femmes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a259e",
   "metadata": {},
   "source": [
    "On regarde ensuite la représentation des hommes et des femmes pour chaque attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9c6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compte_attributs_par_sexe(dataframe, nombre_hommes, nombre_femmes): \n",
    "    \"\"\"Permet de compter le nombre d'hommes et de femmes pour chaque attributs et print les resultats et renvois les attributs qui ont peu de representant\n",
    "    Parameters:\n",
    "        dataframe (pandas.Dataframe): Dataframe contenant les différents attributs par images avec 1 quand l'attribut est présent et -1 quand il ne l'est pas\n",
    "        nombre_hommes (int) : nombre d'hommes dans tout le dataset\n",
    "        nombre_femmes (int) : nombre de femmes dans tout le dataset\n",
    "    Return:\n",
    "        liste_tuples_attributs_nombres(list) : liste de tuples contenant les attributs et une liste du nombre d'hommes et de femmes pour cet attribut\n",
    "    \"\"\"\n",
    "    total_individus = nombre_hommes + nombre_femmes\n",
    "    liste_attributs = dataframe.columns\n",
    "    liste_tuples_attributs_nombres = []\n",
    "\n",
    "    for numero_attributs in range(1,len(liste_attributs)):\n",
    "        attribut = liste_attributs[numero_attributs]\n",
    "        compteur_hommes = 0\n",
    "        compteur_femmes = 0\n",
    "        total_compteur = 0\n",
    "        for num_image in dataframe.index:\n",
    "            if dataframe['Male'][num_image]==1 and dataframe[attribut][num_image]==1:\n",
    "                compteur_hommes += 1\n",
    "            elif dataframe['Male'][num_image]==-1 and dataframe[attribut][num_image]==1:\n",
    "                compteur_femmes += 1\n",
    "        total_compteur = compteur_hommes + compteur_femmes\n",
    "        \n",
    "        print(attribut,\" :\",\"\\n \\t nombre d'hommes: \", compteur_hommes,\"  \",round((compteur_hommes/total_compteur*100),1), \"%\\n \\t nombre de femmes: \",compteur_femmes,\"  \",round((compteur_femmes/total_compteur*100),1),\"%\")\n",
    "        print(\"pourcentage de l'attribut dans le dataset : \",round((total_compteur/total_individus*100),2),\"%\")\n",
    "        \n",
    "        attributs_et_nombres = (attribut, [compteur_hommes, compteur_femmes])\n",
    "        liste_tuples_attributs_nombres.append(attributs_et_nombres)\n",
    "    \n",
    "    return liste_tuples_attributs_nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb59de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_o_Clock_Shadow  : \n",
      " \t nombre d'hommes:  22496    99.9 %\n",
      " \t nombre de femmes:  20    0.1 %\n",
      "pourcentage de l'attribut dans le dataset :  11.11 %\n",
      "Arched_Eyebrows  : \n",
      " \t nombre d'hommes:  4513    8.3 %\n",
      " \t nombre de femmes:  49577    91.7 %\n",
      "pourcentage de l'attribut dans le dataset :  26.7 %\n",
      "Attractive  : \n",
      " \t nombre d'hommes:  23579    22.7 %\n",
      " \t nombre de femmes:  80254    77.3 %\n",
      "pourcentage de l'attribut dans le dataset :  51.25 %\n",
      "Bags_Under_Eyes  : \n",
      " \t nombre d'hommes:  29404    70.9 %\n",
      " \t nombre de femmes:  12042    29.1 %\n",
      "pourcentage de l'attribut dans le dataset :  20.46 %\n",
      "Bald  : \n",
      " \t nombre d'hommes:  4530    99.6 %\n",
      " \t nombre de femmes:  17    0.4 %\n",
      "pourcentage de l'attribut dans le dataset :  2.24 %\n",
      "Bangs  : \n",
      " \t nombre d'hommes:  6950    22.6 %\n",
      " \t nombre de femmes:  23759    77.4 %\n",
      "pourcentage de l'attribut dans le dataset :  15.16 %\n",
      "Big_Lips  : \n",
      " \t nombre d'hommes:  13179    27.0 %\n",
      " \t nombre de femmes:  35606    73.0 %\n",
      "pourcentage de l'attribut dans le dataset :  24.08 %\n",
      "Big_Nose  : \n",
      " \t nombre d'hommes:  35431    74.6 %\n",
      " \t nombre de femmes:  12085    25.4 %\n",
      "pourcentage de l'attribut dans le dataset :  23.45 %\n",
      "Black_Hair  : \n",
      " \t nombre d'hommes:  25156    51.9 %\n",
      " \t nombre de femmes:  23316    48.1 %\n",
      "pourcentage de l'attribut dans le dataset :  23.93 %\n",
      "Blond_Hair  : \n",
      " \t nombre d'hommes:  1749    5.8 %\n",
      " \t nombre de femmes:  28234    94.2 %\n",
      "pourcentage de l'attribut dans le dataset :  14.8 %\n",
      "Blurry  : \n",
      " \t nombre d'hommes:  4833    46.9 %\n",
      " \t nombre de femmes:  5479    53.1 %\n",
      "pourcentage de l'attribut dans le dataset :  5.09 %\n",
      "Brown_Hair  : \n",
      " \t nombre d'hommes:  12788    30.8 %\n",
      " \t nombre de femmes:  28784    69.2 %\n",
      "pourcentage de l'attribut dans le dataset :  20.52 %\n",
      "Bushy_Eyebrows  : \n",
      " \t nombre d'hommes:  20622    71.6 %\n",
      " \t nombre de femmes:  8181    28.4 %\n",
      "pourcentage de l'attribut dans le dataset :  14.22 %\n",
      "Chubby  : \n",
      " \t nombre d'hommes:  10223    87.7 %\n",
      " \t nombre de femmes:  1440    12.3 %\n",
      "pourcentage de l'attribut dans le dataset :  5.76 %\n",
      "Double_Chin  : \n",
      " \t nombre d'hommes:  8315    87.9 %\n",
      " \t nombre de femmes:  1144    12.1 %\n",
      "pourcentage de l'attribut dans le dataset :  4.67 %\n",
      "Eyeglasses  : \n",
      " \t nombre d'hommes:  10478    79.4 %\n",
      " \t nombre de femmes:  2715    20.6 %\n",
      "pourcentage de l'attribut dans le dataset :  6.51 %\n",
      "Goatee  : \n",
      " \t nombre d'hommes:  12703    99.9 %\n",
      " \t nombre de femmes:  13    0.1 %\n",
      "pourcentage de l'attribut dans le dataset :  6.28 %\n",
      "Gray_Hair  : \n",
      " \t nombre d'hommes:  7235    85.1 %\n",
      " \t nombre de femmes:  1264    14.9 %\n",
      "pourcentage de l'attribut dans le dataset :  4.19 %\n",
      "Heavy_Makeup  : \n",
      " \t nombre d'hommes:  234    0.3 %\n",
      " \t nombre de femmes:  78156    99.7 %\n",
      "pourcentage de l'attribut dans le dataset :  38.69 %\n",
      "High_Cheekbones  : \n",
      " \t nombre d'hommes:  25977    28.2 %\n",
      " \t nombre de femmes:  66212    71.8 %\n",
      "pourcentage de l'attribut dans le dataset :  45.5 %\n",
      "Male  : \n",
      " \t nombre d'hommes:  84434    100.0 %\n",
      " \t nombre de femmes:  0    0.0 %\n",
      "pourcentage de l'attribut dans le dataset :  41.68 %\n",
      "Mouth_Slightly_Open  : \n",
      " \t nombre d'hommes:  35846    36.6 %\n",
      " \t nombre de femmes:  62096    63.4 %\n",
      "pourcentage de l'attribut dans le dataset :  48.34 %\n",
      "Mustache  : \n",
      " \t nombre d'hommes:  8414    100.0 %\n",
      " \t nombre de femmes:  3    0.0 %\n",
      "pourcentage de l'attribut dans le dataset :  4.15 %\n",
      "Narrow_Eyes  : \n",
      " \t nombre d'hommes:  10162    43.6 %\n",
      " \t nombre de femmes:  13167    56.4 %\n",
      "pourcentage de l'attribut dans le dataset :  11.51 %\n",
      "No_Beard  : \n",
      " \t nombre d'hommes:  51132    30.2 %\n",
      " \t nombre de femmes:  118026    69.8 %\n",
      "pourcentage de l'attribut dans le dataset :  83.49 %\n",
      "Oval_Face  : \n",
      " \t nombre d'hommes:  18570    32.3 %\n",
      " \t nombre de femmes:  38997    67.7 %\n",
      "pourcentage de l'attribut dans le dataset :  28.41 %\n",
      "Pale_Skin  : \n",
      " \t nombre d'hommes:  2056    23.6 %\n",
      " \t nombre de femmes:  6645    76.4 %\n",
      "pourcentage de l'attribut dans le dataset :  4.29 %\n",
      "Pointy_Nose  : \n",
      " \t nombre d'hommes:  13712    24.4 %\n",
      " \t nombre de femmes:  42498    75.6 %\n",
      "pourcentage de l'attribut dans le dataset :  27.74 %\n",
      "Receding_Hairline  : \n",
      " \t nombre d'hommes:  9913    61.3 %\n",
      " \t nombre de femmes:  6250    38.7 %\n",
      "pourcentage de l'attribut dans le dataset :  7.98 %\n",
      "Rosy_Cheeks  : \n",
      " \t nombre d'hommes:  254    1.9 %\n",
      " \t nombre de femmes:  13061    98.1 %\n",
      "pourcentage de l'attribut dans le dataset :  6.57 %\n",
      "Sideburns  : \n",
      " \t nombre d'hommes:  11438    99.9 %\n",
      " \t nombre de femmes:  11    0.1 %\n",
      "pourcentage de l'attribut dans le dataset :  5.65 %\n",
      "Smiling  : \n",
      " \t nombre d'hommes:  33798    34.6 %\n",
      " \t nombre de femmes:  63871    65.4 %\n",
      "pourcentage de l'attribut dans le dataset :  48.21 %\n",
      "Straight_Hair  : \n",
      " \t nombre d'hommes:  20471    48.5 %\n",
      " \t nombre de femmes:  21751    51.5 %\n",
      "pourcentage de l'attribut dans le dataset :  20.84 %\n",
      "Wavy_Hair  : \n",
      " \t nombre d'hommes:  11892    18.4 %\n",
      " \t nombre de femmes:  52852    81.6 %\n",
      "pourcentage de l'attribut dans le dataset :  31.96 %\n",
      "Wearing_Earrings  : \n",
      " \t nombre d'hommes:  1349    3.5 %\n",
      " \t nombre de femmes:  36927    96.5 %\n",
      "pourcentage de l'attribut dans le dataset :  18.89 %\n",
      "Wearing_Hat  : \n",
      " \t nombre d'hommes:  6871    70.0 %\n",
      " \t nombre de femmes:  2947    30.0 %\n",
      "pourcentage de l'attribut dans le dataset :  4.85 %\n",
      "Wearing_Lipstick  : \n",
      " \t nombre d'hommes:  523    0.5 %\n",
      " \t nombre de femmes:  95192    99.5 %\n",
      "pourcentage de l'attribut dans le dataset :  47.24 %\n",
      "Wearing_Necklace  : \n",
      " \t nombre d'hommes:  1507    6.0 %\n",
      " \t nombre de femmes:  23406    94.0 %\n",
      "pourcentage de l'attribut dans le dataset :  12.3 %\n",
      "Wearing_Necktie  : \n",
      " \t nombre d'hommes:  14697    99.8 %\n",
      " \t nombre de femmes:  35    0.2 %\n",
      "pourcentage de l'attribut dans le dataset :  7.27 %\n",
      "Young  : \n",
      " \t nombre d'hommes:  53447    34.1 %\n",
      " \t nombre de femmes:  103287    65.9 %\n",
      "pourcentage de l'attribut dans le dataset :  77.36 %\n"
     ]
    }
   ],
   "source": [
    "liste_tuples_attributs_nombres = compte_attributs_par_sexe(attributs_images, nombre_hommes, nombre_femmes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744665a",
   "metadata": {},
   "source": [
    "Avec ces informations, un choix des attributs et du nombre de fois où il serait représenté a été fait à la main. Il pourrait être automatisé mais étant donnée qu'il y a un plus grand nombre d'attributs où les hommes sont plus représentés que les femmes, nous avons écarté certains attributs vestimentaires, mais d'autres attributs physiques sont très peu représentés ce qui compliquait l'automatisation en gardant le même nombre d'hommes et de femmes.\n",
    "\n",
    "La fonction *compte_attributs_par_sexe* renvoie cependant une liste de tuples contenant le nom de l'attribut et une liste avec le nombre d'hommes et le nombre de femmes dans le cas où une fonction automatisant la sélection voudrait être développée dans le futur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajout_images_par_attributs(dataframe_complet, nouveau_dataframe_reduit ,liste_attributs, nombre_a_ajoute, mode):\n",
    "    \"\"\"Permet d'ajouter le nombre d'images d'hommes et de femmes avec l'attribut à une liste\n",
    "    Parameters:\n",
    "        dataframe_complet (pandas.Dataframe): Dataframe contenant les différents attributs par images avec 1 quand l'attribut est présent et -1 quand il ne l'est pas\n",
    "        nouveau_dataframe_reduit (pandas.Dataframe) : Dataframe avec les images selectionnées\n",
    "        liste_attributs (list) : liste avec les attributs à ajouter\n",
    "        nombre_a_ajoute (int) : nombre d'image a joute par attributs et par sexe\n",
    "        mode (string) : \"deux\" quand il faut ajouter dans les deux sexe, \"femme\" quand on ajoute que des femmes pour l'attribut, \"homme\" quand on ajoute que des hommes pour l'attribut    \n",
    "    Return:\n",
    "        dataframe_complet (pandas.Dataframe): Dataframe donné en entré avec en moins les lignes des images ajouté dans la liste_numero_images\n",
    "        nouveau_dataframe_reduit (pandas.Dataframe) : Dataframe avec les nouvelles images selectionnées ajouté\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode=='deux':    \n",
    "        for attribut in liste_attributs :\n",
    "            compteur_hommes = 0\n",
    "            compteur_femmes = 0\n",
    "            for num_image in dataframe_complet.index:\n",
    "                \n",
    "                if compteur_hommes == nombre_a_ajoute and compteur_femmes == nombre_a_ajoute : \n",
    "                    break\n",
    "                \n",
    "                if dataframe_complet['Male'][num_image] == 1 and dataframe_complet[attribut][num_image] == 1 and compteur_hommes < nombre_a_ajoute:\n",
    "                    nouveau_dataframe_reduit= nouveau_dataframe_reduit.append(dataframe_complet.loc[num_image])\n",
    "                    dataframe_complet = dataframe_complet.drop([num_image], axis=\"index\")\n",
    "                    compteur_hommes += 1\n",
    "                elif dataframe_complet['Male'][num_image] == -1 and dataframe_complet[attribut][num_image] == 1 and compteur_femmes < nombre_a_ajoute:\n",
    "                    nouveau_dataframe_reduit= nouveau_dataframe_reduit.append(dataframe_complet.loc[num_image])\n",
    "                    dataframe_complet = dataframe_complet.drop([num_image], axis=\"index\")\n",
    "                    compteur_femmes += 1\n",
    "        return dataframe_complet, nouveau_dataframe_reduit\n",
    "            \n",
    "    elif mode == 'femme' or mode == 'homme':\n",
    "        sexe = 1\n",
    "        if mode == 'femme':\n",
    "            sexe = -1\n",
    "        for attribut in liste_attributs :\n",
    "            compteur = 0\n",
    "            for num_image in dataframe_complet.index:\n",
    "                if dataframe_complet['Male'][num_image] == sexe and dataframe_complet[attribut][num_image]==1 and compteur<nombre_a_ajoute:\n",
    "                    nouveau_dataframe_reduit= nouveau_dataframe_reduit.append(dataframe_complet.loc[num_image])\n",
    "                    compteur += 1\n",
    "                    dataframe_complet = dataframe_complet.drop([num_image], axis=\"index\")\n",
    "                if compteur == nombre_a_ajoute:\n",
    "                    break\n",
    "        return dataframe_complet, nouveau_dataframe_reduit\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ea253",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_attributs_hommes_1000 = [\"5_o_Clock_Shadow\", \"Goatee\", \"Sideburns\", \"Wearing_Necktie\"]\n",
    "liste_attributs_hommes_2000 = [\"Bald\", \"Mustache\"]\n",
    "liste_attributs_femmes = [\"Heavy_Makeup\", \"Rosy_Cheeks\", \"Wearing_Earrings\", \"Wearing_Lipstick\"]\n",
    "liste_deux_1000 = [\"Blond_Hair\",  \"Gray_Hair\", \"Pale_Skin\", \"Arched_Eyebrows\",\"Wearing_Hat\", \"Attractive\", \"Bags_Under_Eyes\", \"Bangs\" ,\"Big_Lips\", \"Big_Nose\" , \"Black_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\", \"Eyeglasses\", \"High_Cheekbones\", \"Mouth_Slightly_Open\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pointy_Nose\", \"Receding_Hairline\", \"Smiling\", \"Straight_Hair\", \"Wavy_Hair\", \"Young\"]\n",
    "liste_deux_500 = [\"Chubby\" ,\"Double_Chin\"]\n",
    "\n",
    "nouveau_dataframe_reduit= pds.DataFrame()\n",
    "\n",
    "attributs_images, nouveau_dataframe_reduit = ajout_images_par_attributs(attributs_images, nouveau_dataframe_reduit ,liste_deux_500, 500, \"deux\")\n",
    "attributs_images, nouveau_dataframe_reduit = ajout_images_par_attributs(attributs_images, nouveau_dataframe_reduit ,liste_deux_1000, 1000, \"deux\")\n",
    "attributs_images, nouveau_dataframe_reduit = ajout_images_par_attributs(attributs_images, nouveau_dataframe_reduit ,liste_attributs_hommes_1000, 1000, \"homme\")\n",
    "attributs_images, nouveau_dataframe_reduit = ajout_images_par_attributs(attributs_images, nouveau_dataframe_reduit ,liste_attributs_hommes_2000, 2000, \"homme\")\n",
    "attributs_images, nouveau_dataframe_reduit = ajout_images_par_attributs(attributs_images, nouveau_dataframe_reduit ,liste_attributs_femmes, 2000, \"femme\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouveau_dataframe_reduit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02dc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouveau_dataframe_reduit.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c161d",
   "metadata": {},
   "source": [
    "On melange ensuite la liste et on divise ensuite le dataset de 70000 images équilibré en des dataset plus petit de 1000 images afin de pouvoir entrainer notre modèle en plusieurs fois. En effet, en fonction de l'ordinateur utilisé, un trop grand nombre d'images n'est pas gerable pour l'ordinateur et 1000 images semble pouvoir être gerable pour différente puissance d'ordinateur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  creation_petit_dataset(liste_numero_images, nombre_par_dataset, chemin_sauvegarde) :\n",
    "    \"\"\"Permet la creation de plus petit dataset pour l'entrainement du modèle\n",
    "    Parameters:\n",
    "        liste_numero_images (list) : liste avec les images selectionnées sur lesquels le modèle sera entrainé\n",
    "        nombre_par_dataset (int) : nombre d'images par petit dataset\n",
    "        chemin_sauvegarde (string) : chemin pour la sauvegarde des nouveaux dataset\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    liste_numero_images= liste_numero_images['nb_pic']\n",
    "    liste_numero_images= liste_numero_images.sort_values(0) \n",
    "    liste_data = [liste_numero_images[i:i+1000] for i in range(0,len(liste_numero_images),nombre_par_dataset)]\n",
    "    for i in range(len(liste_data)):\n",
    "        filename=chemin_sauvegarde+ \"/dataset\"+str(i)+\".txt\"\n",
    "        liste_data[i].to_csv(filename, header=None, index=None, sep='\\t', mode='a')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_petit_dataset(nouveau_dataframe_reduit, 1000, \"./small_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b94777",
   "metadata": {},
   "source": [
    "# Création de l'encodeur et du décodeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113dd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,metrics\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(chemin_vers_images, chemin_vers_datalist, largeur, hauteur) :\n",
    "    \"\"\"Charge les images dont le nom est donné par le fichier chemin_vers_datalist, les transforme en numpy array et les mets dans une liste\n",
    "    Parameters:\n",
    "        chemin_vers_images (string) : chemin vers les images d'entrainement du modèle\n",
    "        chemin_vers_datalist (string) : chemin vers les fichiers avec les noms des images avec lesquels le modèle doit être entrainé\n",
    "        largeur (int) : taille en largeur souhaité pour l'image\n",
    "        hauteur (int) : taille en hauteur souhaité pour l'image\n",
    "    Return:\n",
    "        liste_image_pixel (list) : liste de numpy array, chaque numpy array étant une image\n",
    "    \"\"\"\n",
    " \n",
    "    dataset_images = pds.read_csv(chemin_vers_datalist, sep=\"\\t\",header=None,low_memory=False)\n",
    "    liste_nom_image = dataset_images.values.tolist()\n",
    "    liste_image_pixel = []\n",
    "    nombre_images=len(liste_nom_image)\n",
    "    for i in range(0 , nombre_images):\n",
    "        nom=str(liste_nom_image[i])\n",
    "        nom=nom[2:len(nom)-2]\n",
    "        image = Image.open(f'{chemin_vers_images}/{nom}')\n",
    "        image = img.resize((largeur, hauteur))\n",
    "        image = np.array(image)\n",
    "        liste_image_pixel.append(image)\n",
    "    \n",
    "    liste_image_pixel = np.array(liste_image_pixel)\n",
    "    liste_image_pixel = liste_image_pixel.astype('float32') / 255.0 # il faut normaliser pour + de puissance\n",
    "    \n",
    "    return liste_image_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeur_decodeur(largeur, hauteur):\n",
    "    \"\"\"Création de l'autoencodeur qui permettra d'entrainer le modèle, de l'encodeur et du décodeur\n",
    "    Parameters:\n",
    "        largeur (int) : largeur des images\n",
    "        hauteur (int) : hauteur des images\n",
    "    Return:\n",
    "        autoencoder (keras.model) : autoencodeur permettant d'entrainer le modèle\n",
    "        encoder (keras.model) : encodeur permettant d'encoder les images\n",
    "        decoder (keras.model) : decodeur permettant de decoder les images\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = (largeur, hauteur, 3) #taille des input\n",
    "    dropout_level = 0.2 #Dropout level\n",
    "    \n",
    "    # Couche de l'encodeur\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = keras.layers.Conv2D(16, (3, 3),strides=1,activation='relu', padding='same')(input_img)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Conv2D(32, (3, 3),strides=1,activation='relu', padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    encoded = keras.layers.Dense(512,activation='relu')(x)\n",
    "\n",
    "\n",
    "    # Couche du décodeur\n",
    "    x = keras.layers.Dense(512,,\"relu\")(encoded)\n",
    "    x = keras.layers.Dense(8*8*64,\"relu\")(x)\n",
    "    x = keras.layers.Reshape((8, 8, 64))(x)\n",
    "    x = keras.layers.Conv2DTranspose(64,(3,3), activation='relu', padding='same')(x)\n",
    "    x = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = keras.layers.Dropout(dropout_level)(x)\n",
    "    x = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = keras.layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    decoded = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    \n",
    "    nb_decoded_layers = 15 #à ajouter si on ajoute une couche au décodeur\n",
    "    \n",
    "    #Création encoder, autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    encoder = Model(input_img, encoded)\n",
    "    \n",
    "    #Récupération des couches pour le décoder\n",
    "    input_encoded_img = keras.Input(shape=(encoded.shape[1:])) #dimension de l'objet encodé\n",
    "    y = autoencoder.layers[-nb_decoded_layers](input_encoded_img)\n",
    "    \n",
    "    for i in range(nb_decoded_layers-1,0,-1): # 4 3 2 1\n",
    "        y = autoencoder.layers[-i](y)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = Model(input_encoded_img, y)\n",
    "    \n",
    "    # On compile les modèles\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "    encoder.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "    decoder.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "    \n",
    "    return autoencoder,encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(autoencoder):\n",
    "    \"\"\" Plot la loss de l'auto encodeur : si on a un bon modèle, val loss est proche de loss, et on veut une loss faible\n",
    "    Parameters:\n",
    "        autoencoder (keras.model) : autoencodeur contenant l'historique de la loss\n",
    "    \"\"\"\n",
    "\n",
    "    history = autoencoder.history.history\n",
    "\n",
    "    plt.plot(history['val_loss'],label=\"test\")\n",
    "    plt.plot(history['loss'],label=\"training\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_reconstruction(autoencoder,liste_images, largeur, hauteur,n=10):\n",
    "    \"\"\"Affiche l'image original et l'image encoder et decoder par notre autoencodeur de manière à suivre les améliorations du modèle\n",
    "    Parameters:\n",
    "        autoencoder (keras.model) : autoencodeur permettant d'encoder et de decoder les images\n",
    "        liste_images (list) : liste contenant les images sous forme de numpy array \n",
    "        largeur (int) : largeur des images\n",
    "        hauteur (int) : hauteur des images\n",
    "        n (int) : nombre d'images que l'on souhaite afficher, 10 par défault\n",
    "    \"\"\"\n",
    "    \n",
    "    image_decode = autoencoder.predict(liste_images)\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Affiche l'image original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(liste_images[i].reshape(largeur,hauteur,3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Affiche l'image reconstruite\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image_decode[i].reshape(largeur, hauteur,3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66015a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder,encoder,decoder = encodeur_decodeur(128, 128)\n",
    "autoencoder.summary()\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df24f3",
   "metadata": {},
   "source": [
    "L'entrainement étant assez long la fonction *entrainement_modele* permet d'entrainer le modèle en plusieurs fois en veillant à ce qu'ils soient enregistrer à chaque fin d'entrainement dans le cas où le programme est interrompus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def582a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrainement_modeles(autoencoder, encoder, decoder, largeur, hauteur, chemin_modele, nom_modele, chemin_dataset, numero_dataset_debut, numero_dataset_fin ):\n",
    "    \"\"\"Permet d'entrainer le modèle sur un nombre de dataset donnée. A chaque fois que le modèle est entrainer sur un dataset il est sauvegarder afin de le conserver dans le cas où l'ordinateur s'arrete.\n",
    "    Parameters:\n",
    "        autoencoder (keras.model) : autoencodeur permettant d'encoder et de decoder les images\n",
    "        encoder (keras.model) : encodeur permettant d'encoder les images\n",
    "        decoder (keras.model) : decoder permettant de decoder les images\n",
    "        largeur (int) : largeur des images\n",
    "        hauteur (int) : hauteur des images\n",
    "        chemin_model (string) : chemin où l'on souhaite enregistrer le modèle\n",
    "        nom_model (string) : nom que l'on souhaite donner au modèle\n",
    "        chemin_dataset (string) : chemin où les datasets sont enregistré\n",
    "        numero_dataset_debut (int) : numero du dataset où l'on souhaite commencer l'entrainement\n",
    "        numero_dataset_fin (int) : numero du dataset où l'on souhaite arreter l'entrainement\n",
    "    \"\"\"\n",
    "    for num_dataset in range(numero_dataset_debut, numero_dataset_fin): \n",
    "        image_set= chemin_dataset+\"/dataset\"+str(num_dataset)+\".txt\"\n",
    "        liste_image_pixel= load_dataset('img_align_celeba', image_set, largeur, hauteur)\n",
    "\n",
    "        X_train, X_test = train_test_split(liste_image_pixel,\n",
    "                                           test_size=0.2, \n",
    "                                           random_state=0)\n",
    "        del(liste_image_pixel) #On supprime la liste une fois qu'elle a été séparé en deux pour libérer de la mémoire\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=100,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_test, X_test))\n",
    "        plot_loss(autoencoder)\n",
    "        plot_image_reconstruction(autoencoder,X_test , largeur, hauteur)#plot le résultat de l'encodeur pas séparé\n",
    "        del(X_test)\n",
    "        del(X_train)\n",
    "        encoder.save(chemin_modele+\"/encoder_\"+nom_modele)\n",
    "        decoder.save(chemin_modele+\"/decoder_\"+nom_modele)\n",
    "        autoencoder.save(chemin_modele+\"/autoencoder_\"+nom_modele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrainement_modeles(autoencoder, encoder, decoder, 128, 128, \"./Model\", \"vect_512\", \"./small_dataset\", 0, 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122aff9",
   "metadata": {},
   "source": [
    "Si l'on souhaite reprendre l'entrainement aprèsque le programme est été interrompu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad283065",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder=load_model(\"./Model/autoencoder_vect_512\")\n",
    "encoder=load_model(\"./Model/encoder_vect_512\")\n",
    "decoder=load_model(\"./Model/decoder_vect_512\")\n",
    "\n",
    "entrainement_modeles(autoencoder, encoder, decoder, 128, 128, \"./Model\", \"vect_512\", \"./small_dataset\", 35, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd816d",
   "metadata": {},
   "source": [
    "Nous encodons ensuite 50000 qui seront celle que le client pourra choisir lors de la mise en route du logiciel. Encoder les images au préalable permet d'alleger le contenu du logiciel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_images_a_encoder(chemin_vers_images,nombre_image) :\n",
    "    \"\"\"Charge les images qui seront ensuite encoder\n",
    "    Parameters:\n",
    "        chemin_vers_images (string) : chemin vers les images à encoder\n",
    "        nombre_image (int) : nombre d'image à encoder\n",
    "    Return:\n",
    "        liste_image_pixel (list) : liste de numpy array, chaque numpy array étant une image\n",
    "    \"\"\"\n",
    "\n",
    "    img_list = os.listdir(path_to_data)\n",
    "    liste_image_pixel = []\n",
    "\n",
    "    for i in range(nb_im-1000,nb_im):\n",
    "        img = Image.open(f'{path_to_data}/{img_list[i]}')\n",
    "        img = img.resize((128, 128))\n",
    "        img = np.array(img)\n",
    "        liste_image_pixel.append(img)\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        \n",
    "\n",
    "    liste_image_pixel = np.array(liste_image_pixel)\n",
    "    liste_image_pixel = liste_image_pixel.astype('float32') / 255.0 # il faut normaliser pour + de puissance\n",
    "    #img_pixel_list = img_pixel_list.reshape((len(img_pixel_list), np.prod(img_pixel_list.shape[1:])))  # on met ça en vecteur 1D en ligne, mais pas besoin avec conv2D\n",
    "    \n",
    "    return liste_image_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002675d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_encoded_img(nombre_image_a_encoder):\n",
    "    \"\"\"\n",
    "        Enregistre les images encodées\n",
    "    Parameters : \n",
    "        nombre_image_a_encoder (int) : nombre d'image à encoder, doit être un multiple de 1000\n",
    "    \"\"\"\n",
    "    encoder=load_model(\"./Model/encoder_smallset_512_100_8864\")\n",
    "    img_pixel_list= load_images(\"img_align_celeba\",1000)\n",
    "    encoded_img = encoder.predict(img_pixel_list)\n",
    "    print(encoded_img.shape)\n",
    "    nombre_image_a_encoder=nombre_image_a_encoder/1000\n",
    "    \n",
    "    for i in range(2,101):\n",
    "        nb_img= i*1000\n",
    "        img_pixel_list= load_images(\"img_align_celeba\",nb_img)\n",
    "        new_encoded_img = encoder.predict(img_pixel_list)\n",
    "        del(img_pixel_list)\n",
    "        encoded_img=np.vstack([encoded_img,new_encoded_img])\n",
    "        del(new_encoded_img)\n",
    "        print(encoded_img.shape)\n",
    "        \n",
    "    np.save(f\"Data/{len(encoded_img)}_encoded_img\", encoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_encoded_img(nombre_image_a_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
